{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Library imports - multiprocessing version, with filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from dateutil import parser\n",
    "import pytz\n",
    "from timezonefinder import TimezoneFinder\n",
    "\n",
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "from scipy import interpolate\n",
    "import pandas as pd\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time, sys\n",
    "import re\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "#from selenium import webdriver\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "start_time = time.time()\n",
    "    \n",
    "#%matplotlib inline\n",
    "#https://github.com/MrMinimal64/timezonefinder\n",
    "\n",
    "utc=pytz.UTC\n",
    "\n",
    "#debug = True\n",
    "debug = True\n",
    "filepath = './sample_data/'\n",
    "\n",
    "if debug == False:\n",
    "    gpx_file = input('Enter file name (must be in same directory as this program for now): ')\n",
    "    tree = ET.parse(filepath + gpx_file)\n",
    "else:\n",
    "    #tree = ET.parse('./sample_data/Exciter_100.gpx')\n",
    "    #tree = ET.parse('./sample_data/S4_Form_Sprints_20_min_M1_Tempo.gpx')\n",
    "    #tree = ET.parse('./sample_data/ITPS_out.gpx')\n",
    "    #tree = ET.parse('./sample_data/See_you_later_Winter_welcome_Spring.gpx')\n",
    "    tree = ET.parse('./sample_data/wahoo_kickr.tcx')\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "each gps device has its own namespace for recording the GPX file... this function reads the embeded namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def namespace(element):\n",
    "    m = re.match('\\{(.*)\\}', element.tag)\n",
    "    return m.group(1) if m else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = 'http://vortex.plymouth.edu/cgi-bin/sfc/gen-statlog-a.cgi?ident=cyxu&pl=rawspec&yy=19&mm=04&dd=04&pg=web'\n",
    "#dialog http://vortex.plymouth.edu/myo/sfc/statlog-a.html\n",
    "#choose raw hourly & special METAR obs listings\n",
    "#but only valid for us/cn/mx\n",
    "#options\n",
    "#http://www.ogimet.com/metars.phtml.en\n",
    "#http://www.ogimet.com/display_metars2.php?lang=en&lugar=sbsp&tipo=ALL&ord=REV&nil=SI&fmt=html&ano=2019&mes=04&day=04&hora=00&anof=2019&mesf=04&dayf=04&horaf=23&minf=59&send=send\n",
    "\n",
    "#https://www.cyclinganalytics.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#namespace = {'gpx':'http://www.topografix.com/GPX/1/1'}\n",
    "namespace = {'gpx':namespace(root)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(namespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below from:\n",
    "\n",
    "https://towardsdatascience.com/parsing-fitness-tracker-data-with-python-a59e7dc17418\n",
    "\n",
    "and \n",
    "\n",
    "https://github.com/bunburya/fitness_tracker_data_parsing/blob/main/parse_tcx.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Some functions for parsing a TCX file (specifically, a TCX file\n",
    "downloaded from Strava, which was generated based on data recorded by a\n",
    "Garmin vívoactive 3) and creating a Pandas DataFrame with the data.\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, Optional, Any, Union, Tuple\n",
    "\n",
    "import lxml.etree\n",
    "#import pandas as pd\n",
    "import dateutil.parser as dp\n",
    "\n",
    "\n",
    "NAMESPACES = {\n",
    "    'ns': 'http://www.garmin.com/xmlschemas/TrainingCenterDatabase/v2',\n",
    "    'ns2': 'http://www.garmin.com/xmlschemas/UserProfile/v2',\n",
    "    'ns3': 'http://www.garmin.com/xmlschemas/ActivityExtension/v2',\n",
    "    'ns4': 'http://www.garmin.com/xmlschemas/ProfileExtension/v1',\n",
    "    'ns5': 'http://www.garmin.com/xmlschemas/ActivityGoals/v1'\n",
    "}\n",
    "\n",
    "# The names of the columns we will use in our points DataFrame\n",
    "POINTS_COLUMN_NAMES = ['latitude', 'longitude', 'elevation', 'time', 'heart_rate', 'cadence', 'speed', 'lap', 'power']\n",
    "\n",
    "# The names of the columns we will use in our laps DataFrame\n",
    "LAPS_COLUMN_NAMES = ['number', 'start_time', 'distance', 'total_time', 'max_speed', 'max_hr', 'avg_hr']\n",
    "\n",
    "def get_tcx_lap_data(lap: lxml.etree._Element) -> Dict[str, Union[float, datetime, timedelta, int]]:\n",
    "    \"\"\"Extract some data from an XML element representing a lap and\n",
    "    return it as a dict.\n",
    "    \"\"\"\n",
    "    \n",
    "    data: Dict[str, Union[float, datetime, timedelta, int]] = {}\n",
    "    \n",
    "    # Note that because each element's attributes and text are returned as strings, we need to convert those strings\n",
    "    # to the appropriate datatype (datetime, float, int, etc).\n",
    "    \n",
    "    start_time_str = lap.attrib['StartTime']\n",
    "    data['start_time'] = dp.parse(start_time_str)\n",
    "    \n",
    "    distance_elem = lap.find('ns:DistanceMeters', NAMESPACES)\n",
    "    if distance_elem is not None:\n",
    "        data['distance'] = float(distance_elem.text)\n",
    "    \n",
    "    total_time_elem = lap.find('ns:TotalTimeSeconds', NAMESPACES)\n",
    "    if total_time_elem is not None:\n",
    "        data['total_time'] = timedelta(seconds=float(total_time_elem.text))\n",
    "    \n",
    "    max_speed_elem = lap.find('ns:MaximumSpeed', NAMESPACES)\n",
    "    if max_speed_elem is not None:\n",
    "        data['max_speed'] = float(max_speed_elem.text)\n",
    "    \n",
    "    max_hr_elem = lap.find('ns:MaximumHeartRateBpm', NAMESPACES)\n",
    "    if max_hr_elem is not None:\n",
    "        data['max_hr'] = float(max_hr_elem.find('ns:Value', NAMESPACES).text)\n",
    "    \n",
    "    avg_hr_elem = lap.find('ns:AverageHeartRateBpm', NAMESPACES)\n",
    "    if avg_hr_elem is not None:\n",
    "        data['avg_hr'] = float(avg_hr_elem.find('ns:Value', NAMESPACES).text)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def get_tcx_point_data(point: lxml.etree._Element) -> Optional[Dict[str, Union[float, int, str, datetime]]]:\n",
    "    \"\"\"Extract some data from an XML element representing a track point\n",
    "    and return it as a dict.\n",
    "    \"\"\"\n",
    "    \n",
    "    data: Dict[str, Union[float, int, str, datetime]] = {}\n",
    "    \n",
    "    position = point.find('ns:Position', NAMESPACES)\n",
    "    if position is None:\n",
    "        # This Trackpoint element has no latitude or longitude data.\n",
    "        # For simplicity's sake, we will ignore such points.\n",
    "        #return None\n",
    "        data['latitude'] = float(0)\n",
    "        data['longitude'] = float(0)\n",
    "\n",
    "    else:\n",
    "        data['latitude'] = float(position.find('ns:LatitudeDegrees', NAMESPACES).text)\n",
    "        data['longitude'] = float(position.find('ns:LongitudeDegrees', NAMESPACES).text)\n",
    "    \n",
    "    time_str = point.find('ns:Time', NAMESPACES).text\n",
    "    data['time'] = dp.parse(time_str)\n",
    "        \n",
    "    elevation_elem = point.find('ns:AltitudeMeters', NAMESPACES)\n",
    "    if elevation_elem is not None:\n",
    "        data['elevation'] = float(elevation_elem.text)\n",
    "    else:\n",
    "        data['elevation'] = float(0)\n",
    "    \n",
    "    hr_elem = point.find('ns:HeartRateBpm', NAMESPACES)\n",
    "    #print(hr_elem) #debug\n",
    "    if hr_elem is not None:\n",
    "        data['heart_rate'] = int(hr_elem.find('ns:Value', NAMESPACES).text)\n",
    "        \n",
    "    cad_elem = point.find('ns:Cadence', NAMESPACES)\n",
    "    if cad_elem is not None:\n",
    "        data['cadence'] = int(cad_elem.text)\n",
    "    \n",
    "    # The \".//\" here basically tells lxml to search recursively down the tree for the relevant tag, rather than just the\n",
    "    # immediate child elements of speed_elem. See https://lxml.de/tutorial.html#elementpath\n",
    "    speed_elem = point.find('.//ns3:Speed', NAMESPACES)\n",
    "    if speed_elem is not None:\n",
    "        data['speed'] = float(speed_elem.text)\n",
    "        \n",
    "    power_elem = point.find('.//ns3:Watts', NAMESPACES)\n",
    "    if power_elem is not None:\n",
    "        data['power'] = float(power_elem.text)\n",
    "    \n",
    "    return data\n",
    "    \n",
    "\n",
    "def get_dataframes(fname: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Takes the path to a TCX file (as a string) and returns two Pandas\n",
    "    DataFrames: one containing data about the laps, and one containing\n",
    "    data about the individual points.\n",
    "    \"\"\"\n",
    "    \n",
    "    tree = lxml.etree.parse(fname)\n",
    "    root = tree.getroot()\n",
    "    activity = root.find('ns:Activities', NAMESPACES)[0]  # Assuming we know there is only one Activity in the TCX file\n",
    "                                                          # (or we are only interested in the first one)\n",
    "    print('debug', activity) #debug\n",
    "    \n",
    "    points_data = []\n",
    "    laps_data = []\n",
    "    lap_no = 1\n",
    "    for lap in activity.findall('ns:Lap', NAMESPACES):\n",
    "        # Get data about the lap itself\n",
    "        single_lap_data = get_tcx_lap_data(lap)\n",
    "        single_lap_data['number'] = lap_no\n",
    "        laps_data.append(single_lap_data)\n",
    "        \n",
    "        # Get data about the track points in the lap\n",
    "        track = lap.find('ns:Track', NAMESPACES)\n",
    "        point_counter = 0 #debug\n",
    "        for point in track.findall('ns:Trackpoint', NAMESPACES):\n",
    "\n",
    "            single_point_data = get_tcx_point_data(point)\n",
    "            \n",
    "            #print(f'point#{point_counter}') #debug\n",
    "            #print(single_point_data) #debug\n",
    "            #print(point) #debug\n",
    "            point_counter += 1 #debug\n",
    "            \n",
    "            if single_point_data:\n",
    "                single_point_data['lap'] = lap_no\n",
    "                points_data.append(single_point_data)\n",
    "        print(f'collected {point_counter} data points')\n",
    "        lap_no += 1\n",
    "    \n",
    "    # Create DataFrames from the data we have collected. If any information is missing from a particular lap or track\n",
    "    # point, it will show up as a null value or \"NaN\" in the DataFrame.\n",
    "    \n",
    "    laps_df = pd.DataFrame(laps_data, columns=LAPS_COLUMN_NAMES)\n",
    "    laps_df.set_index('number', inplace=True)\n",
    "    points_df = pd.DataFrame(points_data, columns=POINTS_COLUMN_NAMES)\n",
    "    \n",
    "    return laps_df, points_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = './sample_data/wahoo_kickr.tcx'\n",
    "laps_df, points_df = get_dataframes(fname)\n",
    "print('LAPS:')\n",
    "print(laps_df)\n",
    "print('\\nPOINTS:')\n",
    "print(points_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(points_df['power'])\n",
    "plt.plot(points_df['heart_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions\n",
    "def deg2rad(x):\n",
    "    return x * np.pi / 180\n",
    "\n",
    "def rad2deg(angle):\n",
    "    return angle * 180 / np.pi\n",
    "\n",
    "def kt2ms(speed):\n",
    "    return speed * 0.5144444\n",
    "\n",
    "def ms2kmh(speed):\n",
    "    return speed * 3.6\n",
    "\n",
    "def kmh2kt(speed):\n",
    "    return speed / 1.852"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function calculates the distance and angle between 2 coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dist_trk(p2, p1):\n",
    "    #p1 is a list with lat and long in degrees\n",
    "    #p2 is also a list: lat long\n",
    "    #returns distance and angle\n",
    "    #distance in metres\n",
    "    #reference: https://www.movable-type.co.uk/scripts/latlong.html\n",
    "    R = 6371e3 #mean radius of Earth in meters\n",
    "    phi1 = deg2rad(p1[0])\n",
    "    phi2 = deg2rad(p2[0])\n",
    "    lambda1 = deg2rad(p1[1])\n",
    "    lambda2 = deg2rad(p2[1])\n",
    "    delta_phi = phi2 - phi1\n",
    "    delta_lambda = lambda2 - lambda1\n",
    "    a = np.sin(delta_phi / 2) * np.sin(delta_phi / 2) + \\\n",
    "        np.cos(phi1) * np.cos(phi2) * \\\n",
    "        np.sin(delta_lambda / 2) * np.sin(delta_lambda / 2)\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    d = R * c\n",
    "    y = np.sin(delta_lambda) * np.cos(phi2)\n",
    "    x = np.cos(phi1) * np.sin(phi2) - \\\n",
    "        np.sin(phi1) * np.cos(phi2) * np.cos(delta_lambda)\n",
    "    brg = np.arctan2(y,x) * 180 / np.pi\n",
    "    return d, brg + 180"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through GPX file for each recorded track point and calculate distance and angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trk_pts_data(trksegmt):\n",
    "    '''\n",
    "    function to generate track data\n",
    "    input: list of tracksegments\n",
    "    \n",
    "    output: dataframe containing track data\n",
    "    '''\n",
    "    trackpoint = {}\n",
    "    track = pd.DataFrame(columns=['lat', 'lon', 'elev', 'ts', 'dist', 'true_hdg', 'gs', 'delta_t', 'm_power', 'amb_temp', 'heart_rate', 'cadence'])\n",
    "    first_point = True\n",
    "    debug_counter = 0\n",
    "    for idx, trkpoint in enumerate(trksegmt):\n",
    "        trackpoint['lat'] = float(trkpoint.get('lat'))\n",
    "        trackpoint['lon'] = float(trkpoint.get('lon'))\n",
    "        for elev in trkpoint.findall('gpx:ele', namespace):\n",
    "            #print(elev.text)\n",
    "            trackpoint['elev'] = float(elev.text)\n",
    "        for ts in trkpoint.findall('gpx:time', namespace):\n",
    "            #print(ts.text)\n",
    "            trackpoint['ts'] = parser.parse(ts.text)\n",
    "        \n",
    "        #garmin extensions\n",
    "        for extensions in trkpoint.findall('gpx:extensions', namespace):\n",
    "            #print(extensions)\n",
    "            for ext in extensions:\n",
    "                #print(ext.getchildren())\n",
    "                #print(ext.__class__)\n",
    "                #print('power is: ', ext.tag, ext.text)\n",
    "                if 'power' in ext.tag:\n",
    "                    #print('this is power...', float(ext.text))\n",
    "                    trackpoint['m_power'] = float(ext.text)\n",
    "                for child in ext:\n",
    "                    #print(child.tag)\n",
    "                    if 'atemp' in child.tag:\n",
    "                        #print('this is the ambient temp:', float(child.text))\n",
    "                        trackpoint['amb_temp'] = float(child.text)\n",
    "                    if 'hr' in child.tag:\n",
    "                        #print('this is the heart rate:', float(child.text))\n",
    "                        trackpoint['heart_rate'] = float(child.text)\n",
    "                    if 'cad' in child.tag:\n",
    "                        #print('this is the cadence:', float(child.text))\n",
    "                        trackpoint['cadence'] = float(child.text)\n",
    "        \n",
    "        if first_point:\n",
    "            previous_point = trackpoint\n",
    "            first_point = False\n",
    "        dist, angle = get_dist_trk([previous_point['lat'],previous_point['lon']], [trackpoint['lat'],trackpoint['lon']])\n",
    "        if debug_counter < 20:\n",
    "            #print(dist, trackpoint['lon'], previous_point['lon'])\n",
    "            debug_counter += 1\n",
    "\n",
    "        trackpoint['delta_t'] = (trackpoint['ts']-previous_point['ts']).total_seconds()\n",
    "        trackpoint['dist'] = dist\n",
    "        trackpoint['true_hdg'] = angle\n",
    "        if trackpoint['delta_t'] != 0:\n",
    "            trackpoint['gs'] = dist / trackpoint['delta_t']\n",
    "        else:\n",
    "            trackpoint['gs'] = 0\n",
    "            trackpoint['delta_t'] = np.nan\n",
    "            \n",
    "        previous_point = trackpoint\n",
    "        track = track.append(trackpoint, ignore_index=True)\n",
    "        trackpoint = {}\n",
    "    return track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Building track database...')\n",
    "#track = pd.DataFrame(columns=['lat', 'lon', 'elev', 'ts', 'dist', 'true_hdg', 'gs', 'delta_t'])\n",
    "#lat in deg decimal\n",
    "#lon in deg decimal\n",
    "#elev in meters\n",
    "#ts in datetime, tz=0\n",
    "#dist in meters\n",
    "#true_hdg in degrees\n",
    "#gs in m/s\n",
    "\n",
    "debug_counter = 0\n",
    "n_proc = cpu_count() #number of multiprocesses in pool\n",
    "\n",
    "for trk in root.findall('gpx:trk', namespace):\n",
    "    for trksegmt in trk.findall('gpx:trkseg', namespace):\n",
    "        first_point = True\n",
    "        trkpoints = trksegmt.findall('gpx:trkpt', namespace)\n",
    "        trk_pts_number = len(trkpoints)\n",
    "        trk_pts_set = []\n",
    "        \n",
    "        for i in range(n_proc):\n",
    "            if i < (n_proc - 1):\n",
    "                trk_pts_set.append(trkpoints[int(trk_pts_number / n_proc) * (i):(int(trk_pts_number / n_proc) * (i+1) -1)])\n",
    "            else:\n",
    "                trk_pts_set.append(trkpoints[int(trk_pts_number / n_proc) * (i):trk_pts_number])\n",
    "            \n",
    "        #get_trk_pts_data(trk_pts_set[0])\n",
    "        n_proc\n",
    "        pool = Pool(n_proc)\n",
    "        #map guarantees the order back\n",
    "        results = pool.map(get_trk_pts_data, trk_pts_set)\n",
    "        pool.join\n",
    "        #join results:\n",
    "        track = pd.DataFrame(columns=['lat', 'lon', 'elev', 'ts', 'dist', 'true_hdg', 'gs', 'delta_t', 'm_power', 'amb_temp', 'heart_rate', 'cadence'])\n",
    "        for result in results:\n",
    "            track = track.append(result.set_index('ts'))\n",
    "        track['ts'] = track.index\n",
    "        track.reset_index(inplace=True, drop=True)       \n",
    "        pool.terminate\n",
    "        pool.close()\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print()\n",
    "print('Track build time was {:1.1f} seconds'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Butterworth filter definition\n",
    "def apply_filter(raw_data, N, Wn):\n",
    "    \n",
    "    # N is the filter order\n",
    "    #Wn is the 3db cutoff kink point\n",
    "    B, A = signal.butter(N, Wn, output='ba')\n",
    "    \n",
    "    filtered_data = signal.filtfilt(B, A, raw_data)\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the originals for filtering later\n",
    "track['gs_raw'] = track['gs'] \n",
    "track['dist_raw'] = track['dist']\n",
    "track['elev_raw'] = track['elev']\n",
    "\n",
    "#calc delta elevation between points\n",
    "#track['delta_elev'] = track['elev'].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter comparison\n",
    "#track['gs'] = track['gs_raw'].rolling(window=20).mean() * 3.6\n",
    "track['gs'] = apply_filter(track['gs_raw'], 4, 0.04)\n",
    "#conclusion is that the butterworth is better. Smoother signal and maybe less lag\n",
    "track['dist'] = apply_filter(track['dist_raw'], 4, 0.04)\n",
    "track['elev'] = apply_filter(track['elev_raw'], 4, 0.02)\n",
    "track['delta_elev'] = track['elev'].diff()\n",
    "#track['delta_elev'] = apply_filter(track['delta_elev'], 4, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quick plot for checking...\n",
    "plt_list = ['dist', 'gs', 'delta_t', 'elev', 'delta_elev', 'm_power']\n",
    "#plt_list = ['dist', 'gs', 'delta_t', 'elev']\n",
    "plt.rcParams['figure.figsize'] = [16, 12]\n",
    "f, axarr = plt.subplots(len(plt_list), sharex=True)\n",
    "for idx, pltdata in enumerate(plt_list):\n",
    "    axarr[idx].plot((track['ts']),track[pltdata])\n",
    "    axarr[idx].set(ylabel=pltdata,\n",
    "       title=pltdata)\n",
    "    axarr[idx].grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pandas\n",
    "#shift\n",
    "#apply\n",
    "#applymap\n",
    "#run apply on column and use lambda to filter the 2 data points and pass to function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the mean LAT / LON\n",
    "\n",
    "Find weather stations near the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radial_distance = 100 #nm\n",
    "print(f'Figuring out nearest weather station...using {radial_distance} nm search radius...')\n",
    "#nearest airport\n",
    "#http://aviationweather.gov/adds/dataserver_current/httpparam?dataSource=stations&requestType=retrieve&format=xml&radialDistance=20;-81.275376,43.024538\n",
    "lon_mean = track['lon'].mean()\n",
    "lat_mean = track['lat'].mean()\n",
    "ride_day = track['ts'][0].day\n",
    "ride_month = track['ts'][0].month\n",
    "ride_year = track['ts'][0].year\n",
    "\n",
    "\n",
    "\n",
    "url = 'http://aviationweather.gov/adds/dataserver_current/httpparam?dataSource=stations&requestType=retrieve' + \\\n",
    "      '&format=xml&radialDistance=' + str(radial_distance) + ';' + str(lon_mean) + ',' + str(lat_mean)\n",
    "\n",
    "source = urllib.request.urlopen(url).read()\n",
    "tree = ET.ElementTree(ET.fromstring(source))\n",
    "root = tree.getroot()\n",
    "station_id_list = []\n",
    "for data in root.findall('data'):\n",
    "    for station in data.findall('Station'):\n",
    "        for station_ident in station.findall('station_id'):\n",
    "            station_id_list.append(station_ident.text)\n",
    "print('Nearest stations are: ', station_id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cycle each station and try to find a METAR\n",
    "\n",
    "If found, then create the weather database\n",
    "\n",
    "If not found, asks user to enter prevailing wind speed and direction (true magnetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lat_mean)\n",
    "print(lon_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "other sites from this thread:\n",
    "http://www.redemet.aer.mil.br/api/consulta_automatica/index.php?local=sbgp&msg=metar&data_ini=2020052201&data_fim=2020052223\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_redemet_data(station, ride_day, ride_month, ride_year):\n",
    "    '''\n",
    "    retrieve weather data from REDEMET\n",
    "    this site only offers BRASIL\n",
    "    inputs:\n",
    "        station: (str) - 4 letter code\n",
    "        ride_day, ride_month, ride_year: (int)\n",
    "    returns:\n",
    "        found_data: (bool) true if data was found\n",
    "        metinfo: (list) - parsed data\n",
    "        \n",
    "    site url template:\n",
    "    https://www.redemet.aer.mil.br/?i=facilidades&p=api-redemet\n",
    "    http://www.redemet.aer.mil.br/api/consulta_automatica/index.php?local=sbgp&msg=metar&data_ini=2020052201&data_fim=2020052223\n",
    "    '''\n",
    "\n",
    "    #only try to retrieve data if station is not null\n",
    "    metar_found = False\n",
    "    metar_db = []\n",
    "    if station != '':\n",
    "        url = f'http://www.redemet.aer.mil.br/api/consulta_automatica/index.php?local={station}'+ \\\n",
    "        f'&msg=metar&data_ini={ride_year}{ride_month:02d}{ride_day:02d}00'+ \\\n",
    "        f'&data_fim={ride_year}{ride_month:02d}{ride_day:02d}23' #from 00h to 23h on that day\n",
    "        page = urllib.request.urlopen(url).read()\n",
    "        soup = bs.BeautifulSoup(page)\n",
    "        body = soup.body\n",
    "        lines = body.get_text().splitlines()\n",
    "        for line in lines:\n",
    "            #parse list with metars\n",
    "            #possible lines:\n",
    "            #\"2020052213 - Mensagem METAR de 'SBGP' para 22/05/2020 as 13(UTC) não localizada na base de dados da REDEMET\",\n",
    "            #'2020052214 - METAR SBGP 221400Z 03020KT CAVOK 26/17 Q1019=',\n",
    "            w_offset = 0 #offset for VRB wind speed\n",
    "            if 'localizada' in line:\n",
    "                pass\n",
    "            else:\n",
    "                metar_found = True\n",
    "                line_items = line.split(' ')\n",
    "                #let's split the items now\n",
    "                #['2020052214', '-', 'METAR', 'SBGP', '221400Z', '03020KT', 'CAVOK', '26/17', 'Q1019=']\n",
    "                mdate = line_items[4]\n",
    "                mwind = line_items[5]\n",
    "                mday = int(mdate[:2])\n",
    "                mhour = int(mdate[2:4])\n",
    "                mminute = int(mdate[4:6])\n",
    "                if mwind[:2] == 'VR':\n",
    "                    wind_dir = 0\n",
    "                    w_offset = 1\n",
    "                else:\n",
    "                    wind_dir = int(mwind[:2])\n",
    "                wind_spd = int(mwind[2+w_offset:5])\n",
    "                metar_station = line_items[3]\n",
    "                metar_db.append([datetime.datetime(year=ride_year, month=ride_month, day=ride_day, \n",
    "                                                           hour=mhour, minute=mminute, second=0, tzinfo=pytz.UTC), \n",
    "                                                           wind_dir, wind_spd])\n",
    "\n",
    "        weather_data = pd.DataFrame(columns=['Time', 'Wind_Dir', 'Wind Speed'] , data=metar_db)\n",
    "        weather_data['Time'] = pd.to_datetime(weather_data['Time'])\n",
    "        weather_data.sort_values(by='Time',ascending=True, inplace=True)\n",
    "        weather_data.set_index('Time')\n",
    "        return metar_found, weather_data\n",
    "    else:\n",
    "        return False, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ogi_data(station, ride_day, ride_month, ride_year):\n",
    "    '''\n",
    "    retrieve weather data from OGIMET\n",
    "    this site only offers the last 30 days\n",
    "    inputs:\n",
    "        station: (str) - 4 letter code\n",
    "        ride_day, ride_month, ride_year: (int)\n",
    "    returns:\n",
    "        found_data: (bool) true if data was found\n",
    "        metinfo: (list) - parsed data\n",
    "        \n",
    "    site url template:\n",
    "    http://www.ogimet.com/display_metars2.php?lang=en&amp;lugar=CYXU&amp;tipo=ALL&amp;ord=REV&amp;nil=SI&amp;fmt=html&amp;ano=2020&amp;mes=5&amp;day=16&amp;hora=00&amp;anof=2020&amp;mesf=5&amp;dayf=16&amp;horaf=23&amp;minf=59&amp;send=send\n",
    "    http://www.ogimet.com/display_metars2.php?lang=en&lugar=CYXU&tipo=ALL&ord=REV&nil=SI&fmt=html&ano=2020&mes=05&day=16&hora=21&anof=2020&mesf=05&dayf=16&horaf=21&minf=59&send=send\n",
    "    '''\n",
    "\n",
    "    #only try to retrieve data if station is not null\n",
    "    metar_found = False\n",
    "    metar_db = []\n",
    "    if station != '':\n",
    "        url = 'http://www.ogimet.com/display_metars2.php?lang=en&lugar='+str(station)+ \\\n",
    "        '&tipo=ALL&ord=REV&nil=SI&fmt=html&ano='+str(ride_year)+ \\\n",
    "        '&mes='+str(ride_month)+'&day='+str(ride_day)+ \\\n",
    "        '&hora=00&anof='+str(ride_year)+'&mesf='+str(ride_month)+'&dayf='+str(ride_day)+ \\\n",
    "        '&horaf=23&minf=59&send=send'\n",
    "        source = urllib.request.urlopen(url).read()\n",
    "        soup = bs.BeautifulSoup(source, features='html.parser')\n",
    "        table = soup.table\n",
    "        table_rows = table.find_all('tr')\n",
    "        metar_list = []\n",
    "        for tr in table_rows:\n",
    "            td = tr.find_all('td')\n",
    "            row = [i.text for i in td]\n",
    "            metar_list.append(row)\n",
    "        for idx, line in enumerate(metar_list):\n",
    "            #print(idx, line)\n",
    "            for item in line:\n",
    "                found = item.find('METAR '+str(station))\n",
    "                if found != -1:\n",
    "                    metar_found = True\n",
    "                    try:\n",
    "                        mday = int(item[found+11:found+13])\n",
    "                        mhour = int(item[found+13:found+15])\n",
    "                        mminute = int(item[found+15:found+17])\n",
    "                        wind_dir = int(item[found+19:found+22])\n",
    "                        wind_spd = int(item[found+22:found+24])\n",
    "                        metar_station = str(station)\n",
    "                        #print('mday:', mday)\n",
    "                        #print('mhour:', mhour)\n",
    "                        #print('mminute:', mminute)\n",
    "                        #print('wind_dir:', wind_dir)\n",
    "                        #print('wind_spd:', wind_spd)\n",
    "\n",
    "                        metar_db.append([datetime.datetime(year=ride_year, month=ride_month, day=ride_day, \n",
    "                                                           hour=mhour, minute=mminute, second=0, tzinfo=pytz.UTC), \n",
    "                                                           wind_dir, wind_spd])\n",
    "                        #wind speed is in knots, default of a METAR\n",
    "\n",
    "                    except:\n",
    "                        #print('parsing error')\n",
    "                        pass\n",
    "        #naming columns to harmonize with wunder...not my choice\n",
    "        weather_data = pd.DataFrame(columns=['Time', 'Wind_Dir', 'Wind Speed'] , data=metar_db)\n",
    "        weather_data['Time'] = pd.to_datetime(weather_data['Time'])\n",
    "        weather_data.sort_values(by='Time',ascending=True, inplace=True)\n",
    "        weather_data.set_index('Time')\n",
    "        return metar_found, weather_data\n",
    "    else:\n",
    "        return False, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wunder_data(station, ride_day, ride_month, ride_year):\n",
    "    '''\n",
    "    retrieve weather data from WUNDERGROUND\n",
    "    inputs:\n",
    "        station: (str) - 4 letter code\n",
    "        ride_day, ride_month, ride_year: (int)\n",
    "    returns:\n",
    "        found_data: (bool) true if data was found\n",
    "        metinfo: (pandas.df) - parsed data\n",
    "        \n",
    "    site url template:\n",
    "      https://www.wunderground.com/history/daily/CYXU/date/2004-5-10\n",
    "    '''\n",
    "    \n",
    "    url = 'https://www.wunderground.com/history/daily/'+str(station)+ \\\n",
    "    '/date/' + str(ride_year) + '-' + str(ride_month) + '-'+str(ride_day)\n",
    "\n",
    "    # instantiate webdriver and get data\n",
    "    # you need firefox bynary and the geckodriver installed\n",
    "    # the geckodriver interfaces with firefox to get the page and pass the data back\n",
    "    wd = webdriver.Firefox()\n",
    "    wd.get(url)\n",
    "\n",
    "    soup = bs.BeautifulSoup(wd.page_source, 'lxml')\n",
    "\n",
    "    wd.quit()\n",
    "    \n",
    "    #find all tables\n",
    "    \n",
    "    data_found = False\n",
    "    table_index = 0\n",
    "    tables = soup.find_all('table')\n",
    "    \n",
    "    #find the correct table\n",
    "    for tidx, table in enumerate(tables):\n",
    "        table_head = table.findAll('th')\n",
    "        output_head = []\n",
    "        for head in table_head:\n",
    "            output_head.append(head.text.strip())\n",
    "            if output_head[0] == 'Time':\n",
    "                data_found = True\n",
    "                table_index = tidx\n",
    "    #get the data\n",
    "    \n",
    "    if not data_found:\n",
    "        #if no tables found, return false and an None\n",
    "        return data_found, None\n",
    "    else:\n",
    "\n",
    "        table = tables[table_index]\n",
    "\n",
    "        # parse table - first the header\n",
    "        table_head = table.findAll('th')\n",
    "        output_head = []\n",
    "        for head in table_head:\n",
    "            output_head.append(head.text.strip())\n",
    "\n",
    "        # create empty dataframe\n",
    "        weather_data = pd.DataFrame(columns=output_head)\n",
    "\n",
    "        # parse rows\n",
    "        output_rows = []\n",
    "        rows = table.findAll('tr')\n",
    "        for row in rows:\n",
    "            columns = row.findAll('td')\n",
    "            clean_row = []\n",
    "            for column in columns:\n",
    "                clean_row.append(column.text.strip().replace(u'\\xa0', '') #this is to remove unwanted characters\n",
    "                                 .replace('mph', '') \n",
    "                                 .replace('F', '')\n",
    "                                 .replace('in', ''))\n",
    "            if len(clean_row) > 0: # discard row if it is empty\n",
    "                output_rows.append(clean_row)\n",
    "\n",
    "        # add it all to the dataframe\n",
    "        weather_data = pd.DataFrame(columns=output_head, data=output_rows).apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "        # substitute wind direction for numeric headings\n",
    "        wind_dir_dict = {\"N\":0,\"NNE\":22.5,\"NE\":45,\"ENE\":67.5,\n",
    "                         \"E\":90,\"ESE\":112.5, \"SE\":135, \"SSE\":157.5,\n",
    "                         \"S\":180,\"SSW\":202.5,\"SW\":225,\"WSW\":247.5,\n",
    "                         \"W\":270,\"WNW\":292.5,\"NW\":315,\"NNW\":337.5, \n",
    "                         \"CALM\":0, \"VAR\":0}\n",
    "\n",
    "        weather_data['Wind_Dir'] = weather_data['Wind'].map(wind_dir_dict)\n",
    "\n",
    "        #make time something useful\n",
    "        weather_data['Time'] = pd.to_datetime(weather_data['Time'].apply(lambda x : str(ride_year)+\n",
    "                                                                         '-'+str(ride_month)+\n",
    "                                                                         '-'+str(ride_day)+\n",
    "                                                                         ' '+x))\n",
    "        #sort\n",
    "        weather_data.sort_values(by='Time',ascending=True, inplace=True)\n",
    "        #harmonize and keep wind speed in kts\n",
    "        weather_data['Wind Speed'] = kmh2kt(weather_data['Wind Speed'])\n",
    "        print()\n",
    "        print('returning this weather data:')\n",
    "        print(weather_data)\n",
    "        \n",
    "        return data_found, weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metinfo_found = False\n",
    "datetime.date.today()\n",
    "delta_days = (datetime.date.today() - (track['ts'][0].to_pydatetime()).date()).days\n",
    "\n",
    "station_index = 0\n",
    "while metinfo_found == False and station_index < len(station_id_list):\n",
    "\n",
    "    if delta_days < 30:\n",
    "        #try to get data from ogimet\n",
    "        #if successful set metinfo_found = True\n",
    "        print(f\"Checking if I can find data for station {station_id_list[station_index]} at OGI...\", end='')\n",
    "        \n",
    "        if station_id_list[station_index]!= '':\n",
    "            metinfo_found, metinfo_db = get_ogi_data(station_id_list[station_index], ride_day, ride_month, ride_year)\n",
    "        if metinfo_found:\n",
    "            print(\"found data!\")\n",
    "        else:\n",
    "            print(\"no\")\n",
    "            \n",
    "    if (not metinfo_found) and ('SB' in station_id_list[station_index]):\n",
    "        print(f\"Checking if I can find data for station {station_id_list[station_index]} at REDEMET...\", end='')\n",
    "        \n",
    "        if station_id_list[station_index]!= '':\n",
    "            metinfo_found, metinfo_db = get_redemet_data(station_id_list[station_index], ride_day, ride_month, ride_year)\n",
    "        if metinfo_found:\n",
    "            print(\"found data!\")\n",
    "        else:\n",
    "            print(\"no\")\n",
    "        \n",
    "    if not metinfo_found:\n",
    "        #try to get data from wunder\n",
    "        #if successful, set metinfo_found = True\n",
    "        print(f\"Checking if I can find data for station {station_id_list[station_index]} at Wunder...\", end='')\n",
    "        \n",
    "        if station_id_list[station_index]!= '':\n",
    "            metinfo_found, metinfo_db = get_wunder_data(station_id_list[station_index], ride_day, ride_month, ride_year)\n",
    "        if metinfo_found:\n",
    "            #we neeed to shift the time because Wunder returns local time. We need UTC\n",
    "            tf = TimezoneFinder()\n",
    "            ride_tz = tf.timezone_at(lng=lon_mean, lat=lat_mean)\n",
    "            metinfo_db['Time'] = pd.to_datetime(metinfo_db['Time']).dt.tz_localize(ride_tz).dt.tz_convert('UTC')\n",
    "            \n",
    "            print(\"found data!\")\n",
    "        else:\n",
    "            print(\"no\")\n",
    "            \n",
    "    station_index += 1\n",
    "if not metinfo_found:\n",
    "    #manual entry\n",
    "    print('no auto METAR data')\n",
    "    wind_dir = input('Enter wind direction (true heading): ')\n",
    "    wind_spd = input('Enter wind speed: ')\n",
    "    ride_date = datetime.datetime(year=ride_year, month=ride_month, day=ride_day, hour=0, minute=0, second=0, tzinfo=pytz.UTC)\n",
    "    metinfo_db = pd.DataFrame({'Time':ride_date, 'Wind_Dir':int(wind_dir), 'Wind Speed':float(wind_spd)}, index=[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculates the headwind component and stores in database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(station_id_list, ride_day, ride_month, ride_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metinf_db['Time'] = pd.to_datetime(metinfo_db['Time'])\n",
    "metinfo_db\n",
    "#print(track['ts'].iloc[0], track['ts'].iloc[-1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx=0\n",
    "yy=-1\n",
    "print(track['ts'].iloc[xx])\n",
    "print(metinfo_db['Time'].iloc[yy])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "print('Populate wind components...')\n",
    "#create columns\n",
    "track['wind_spd'] = 0\n",
    "track['wind_dir'] = 0\n",
    "\n",
    "#find start\n",
    "if metinfo_db.shape[0] == 1: #only one wind data point, easy\n",
    "    track['wind_spd'] = metinfo_db['Wind Speed'][0]\n",
    "    track['wind_dir'] = metinfo_db['Wind_Dir'][0]\n",
    "else:\n",
    "    #find and set values\n",
    "    '''\n",
    "    cases that will happen:\n",
    "    1) last METAR < first track ts\n",
    "    all track timestamps are > than metar ts\n",
    "    2) first METAR ts > first track ts\n",
    "    3) last METAR ts < last track ts\n",
    "    4) first METAR ts < first track ts (trivial)\n",
    "    5) last METAR ts > last track ts (trivial)\n",
    "    '''\n",
    "    #track earlier than metar\n",
    "    track.loc[(track['ts'] <= metinfo_db['Time'].iloc[0]), ['wind_spd']] = metinfo_db['Wind Speed'].iloc[0]\n",
    "    track.loc[(track['ts'] <= metinfo_db['Time'].iloc[0]), ['wind_dir']] = metinfo_db['Wind_Dir'].iloc[0]\n",
    "    #track later than metar\n",
    "    track.loc[(track['ts'] >= metinfo_db['Time'].iloc[-1]), ['wind_spd']] = metinfo_db['Wind Speed'].iloc[-1]\n",
    "    track.loc[(track['ts'] >= metinfo_db['Time'].iloc[-1]), ['wind_dir']] = metinfo_db['Wind_Dir'].iloc[-1]\n",
    "    \n",
    "    #track in between\n",
    "    index_start = metinfo_db[track['ts'].iloc[0] <= metinfo_db['Time']].index[0]\n",
    "    index_end = metinfo_db[track['ts'].iloc[-1] < metinfo_db['Time']].index[0]\n",
    "    \n",
    "    metinfo_indexes = range(index_start, index_end + 1)\n",
    "    for metinfo_index in metinfo_indexes:\n",
    "        if metinfo_index == index_start:\n",
    "            track.loc[track['ts'] <= metinfo_db['Time'].iloc[metinfo_index], ['wind_spd']] = metinfo_db['Wind Speed'].iloc[metinfo_index]\n",
    "            track.loc[track['ts'] <= metinfo_db['Time'].iloc[metinfo_index], ['wind_dir']] = metinfo_db['Wind_Dir'].iloc[metinfo_index]\n",
    "\n",
    "        #elif metinfo_index < index_end:\n",
    "        else:\n",
    "            track.loc[(track['ts'] <= metinfo_db['Time'].iloc[metinfo_index]) & \\\n",
    "                       (track['ts'] > metinfo_db['Time'].iloc[metinfo_index - 1]), ['wind_spd']] = metinfo_db['Wind Speed'].iloc[metinfo_index]\n",
    "            track.loc[(track['ts'] <= metinfo_db['Time'].iloc[metinfo_index]) & \\\n",
    "                       (track['ts'] > metinfo_db['Time'].iloc[metinfo_index - 1]), ['wind_dir']] = metinfo_db['Wind_Dir'].iloc[metinfo_index]\n",
    "\n",
    "        #else:\n",
    "        #    track.loc[track['ts'] > metinfo_db['Time'].iloc[metinfo_index], ['wind_spd']] = metinfo_db['Wind Speed'].iloc[metinfo_index]\n",
    "        #    track.loc[track['ts'] > metinfo_db['Time'].iloc[metinfo_index], ['wind_dir']] = metinfo_db['Wind_Dir'].iloc[metinfo_index]\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [16, 8]\n",
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].plot((track['ts']), track['wind_spd'])\n",
    "\n",
    "ax[0].set(xlabel='Time', ylabel='wind speed [kt]')\n",
    "ax[0].grid()\n",
    "\n",
    "ax[1].plot((track['ts']), track['wind_dir'])\n",
    "\n",
    "ax[1].set(xlabel='Time', ylabel='wind dir')\n",
    "ax[1].grid()\n",
    "\n",
    "#fig.savefig(\"test.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Populate wind components...spline version')\n",
    "#create columns\n",
    "track['wind_spd'] = 0\n",
    "track['wind_dir'] = 0\n",
    "\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "x = metinfo_db['Time'].values.astype('uint64') / 1e6\n",
    "y = metinfo_db['Wind Speed']\n",
    "wss = UnivariateSpline(x, y, s=0.5)\n",
    "\n",
    "y = metinfo_db['Wind_Dir']\n",
    "wds = UnivariateSpline(x, y, s=0.5)\n",
    "\n",
    "\n",
    "#find start\n",
    "if metinfo_db.shape[0] == 1: #only one wind data point, easy\n",
    "    track['wind_spd'] = metinfo_db['Wind Speed'][0]\n",
    "    track['wind_dir'] = metinfo_db['Wind_Dir'][0]\n",
    "else:\n",
    "    #find and set values\n",
    "    '''\n",
    "    cases that will happen:\n",
    "    1) last METAR < first track ts\n",
    "    all track timestamps are > than metar ts\n",
    "    2) first METAR ts > first track ts\n",
    "    3) last METAR ts < last track ts\n",
    "    4) first METAR ts < first track ts (trivial)\n",
    "    5) last METAR ts > last track ts (trivial)\n",
    "    '''\n",
    "    #track earlier than metar\n",
    "    track.loc[(track['ts'] <= metinfo_db['Time'].iloc[0]), ['wind_spd']] = metinfo_db['Wind Speed'].iloc[0]\n",
    "    track.loc[(track['ts'] <= metinfo_db['Time'].iloc[0]), ['wind_dir']] = metinfo_db['Wind_Dir'].iloc[0]\n",
    "    #track later than metar\n",
    "    track.loc[(track['ts'] >= metinfo_db['Time'].iloc[-1]), ['wind_spd']] = metinfo_db['Wind Speed'].iloc[-1]\n",
    "    track.loc[(track['ts'] >= metinfo_db['Time'].iloc[-1]), ['wind_dir']] = metinfo_db['Wind_Dir'].iloc[-1]\n",
    "    \n",
    "    #track in between\n",
    "    index_start = metinfo_db[track['ts'].iloc[0] <= metinfo_db['Time']].index[0]\n",
    "    index_end = metinfo_db[track['ts'].iloc[-1] < metinfo_db['Time']].index[0]\n",
    "    \n",
    "    metinfo_indexes = range(index_start, index_end + 1)\n",
    "    for metinfo_index in metinfo_indexes:\n",
    "        if metinfo_index == index_start:\n",
    "            track.loc[track['ts'] <= metinfo_db['Time'].iloc[metinfo_index], ['wind_spd']] = metinfo_db['Wind Speed'].iloc[metinfo_index]\n",
    "            track.loc[track['ts'] <= metinfo_db['Time'].iloc[metinfo_index], ['wind_dir']] = metinfo_db['Wind_Dir'].iloc[metinfo_index]\n",
    "            \n",
    "        else:\n",
    "            track.loc[(track['ts'] <= metinfo_db['Time'].iloc[metinfo_index]) & \\\n",
    "                       (track['ts'] > metinfo_db['Time'].iloc[metinfo_index - 1]), ['wind_spd']] = \\\n",
    "                       wss(track.loc[(track['ts'] <= metinfo_db['Time'].iloc[metinfo_index]) & \\\n",
    "                       (track['ts'] > metinfo_db['Time'].iloc[metinfo_index - 1]), ['ts']].iloc[:,0].values.astype('uint64') / 1e6)\n",
    "            track.loc[(track['ts'] <= metinfo_db['Time'].iloc[metinfo_index]) & \\\n",
    "                       (track['ts'] > metinfo_db['Time'].iloc[metinfo_index - 1]), ['wind_dir']] = \\\n",
    "                       wds(track.loc[(track['ts'] <= metinfo_db['Time'].iloc[metinfo_index]) & \\\n",
    "                       (track['ts'] > metinfo_db['Time'].iloc[metinfo_index - 1]), ['ts']].iloc[:,0].values.astype('uint64') / 1e6)\n",
    "\n",
    "        #else:\n",
    "        #    track.loc[track['ts'] > metinfo_db['Time'].iloc[metinfo_index], ['wind_spd']] = metinfo_db['Wind Speed'].iloc[metinfo_index]\n",
    "        #    track.loc[track['ts'] > metinfo_db['Time'].iloc[metinfo_index], ['wind_dir']] = metinfo_db['Wind_Dir'].iloc[metinfo_index]\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [16, 8]\n",
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].plot((track['ts']), track['wind_spd'])\n",
    "\n",
    "ax[0].set(xlabel='Time', ylabel='wind speed [kt]')\n",
    "ax[0].grid()\n",
    "\n",
    "ax[1].plot((track['ts']), track['wind_dir'])\n",
    "\n",
    "ax[1].set(xlabel='Time', ylabel='wind dir')\n",
    "ax[1].grid()\n",
    "\n",
    "#fig.savefig(\"test.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "print('Populate wind components...')\n",
    "#create columns\n",
    "track['wind_spd'] = 0\n",
    "track['wind_dir'] = 0\n",
    "\n",
    "#find start\n",
    "if metinfo_db.shape[0] == 1:\n",
    "    index_start = metinfo_db.index[0]\n",
    "    index_end = metinfo_db.index[0]\n",
    "else:\n",
    "    index_start = metinfo_db[track['ts'].iloc[0] <= metinfo_db['Time']].index[0]\n",
    "    index_end = metinfo_db[track['ts'].iloc[-1] < metinfo_db['Time']].index[0]\n",
    "\n",
    "#find and set values\n",
    "\n",
    "cases that will happen:\n",
    "1) last METAR < first track ts\n",
    "all track timestamps are > than metar ts\n",
    "2) first METAR ts > first track ts\n",
    "3) last METAR ts < last track ts\n",
    "4) first METAR ts < first track ts (trivial)\n",
    "5) last METAR ts > last track ts (trivial)\n",
    "\n",
    "\n",
    "metinfo_indexes = range(index_start, index_end)\n",
    "for metinfo_index in range(index_start, index_end+1):\n",
    "    if metinfo_index == index_start:\n",
    "        track.loc[track['ts'] <= metinfo_db['Time'].iloc[metinfo_index], ['wind_spd']] = metinfo_db['Wind Speed'].iloc[metinfo_index]\n",
    "        track.loc[track['ts'] <= metinfo_db['Time'].iloc[metinfo_index], ['wind_dir']] = metinfo_db['Wind_Dir'].iloc[metinfo_index]\n",
    "\n",
    "    elif metinfo_index < index_end:\n",
    "        track.loc[(track['ts'] <= metinfo_db['Time'].iloc[metinfo_index]) & \\\n",
    "                   (track['ts'] > metinfo_db['Time'].iloc[metinfo_index - 1]), ['wind_spd']] = metinfo_db['Wind Speed'].iloc[metinfo_index]\n",
    "        track.loc[(track['ts'] <= metinfo_db['Time'].iloc[metinfo_index]) & \\\n",
    "                   (track['ts'] > metinfo_db['Time'].iloc[metinfo_index - 1]), ['wind_dir']] = metinfo_db['Wind_Dir'].iloc[metinfo_index]\n",
    "\n",
    "    else:\n",
    "        track.loc[track['ts'] > metinfo_db['Time'].iloc[metinfo_index], ['wind_spd']] = metinfo_db['Wind Speed'].iloc[metinfo_index]\n",
    "        track.loc[track['ts'] > metinfo_db['Time'].iloc[metinfo_index], ['wind_dir']] = metinfo_db['Wind_Dir'].iloc[metinfo_index]\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [16, 8]\n",
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].plot((track['ts']), track['wind_spd'])\n",
    "\n",
    "ax[0].set(xlabel='Time', ylabel='wind speed [kt]')\n",
    "ax[0].grid()\n",
    "\n",
    "ax[1].plot((track['ts']), track['wind_dir'])\n",
    "\n",
    "ax[1].set(xlabel='Time', ylabel='wind dir')\n",
    "ax[1].grid()\n",
    "\n",
    "#fig.savefig(\"test.png\")\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Calculating head winds...')\n",
    "\n",
    "#delta_ang in degrees\n",
    "#wind_component in m/s\n",
    "track['delta_ang'] = track['true_hdg'] - track['wind_dir']\n",
    "track['headwind_comp'] = np.cos(deg2rad(track['delta_ang'])) * kt2ms(track['wind_spd'])\n",
    "track['airspeed'] = track['gs'] + track['headwind_comp'] #this in ms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculates DRAG the power\n",
    "\n",
    "Todo: interface for Cd and Area; Vary rho per altitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.cut(track['airspeed'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crouch_cd(x, crouch_cd_interp, crouch_start, crouch_full):\n",
    "    if x <= crouch_start:\n",
    "        return 1\n",
    "    elif x <= crouch_full:\n",
    "        return crouch_cd_interp(x)\n",
    "    else:\n",
    "        return crouch_cd_interp(crouch_full)\n",
    "    \n",
    "def crouch_S(x, crouch_S_interp, crouch_start, crouch_full):\n",
    "    if x <= crouch_start:\n",
    "        return 1\n",
    "    elif x <= crouch_full:\n",
    "        return crouch_S_interp(x)\n",
    "    else:\n",
    "        return crouch_S_interp(crouch_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Calculating aero power...')\n",
    "rho = 1.225 #fixed for now kg/m3\n",
    "Cd = 0.9 #fixed for me\n",
    "S = 0.516 #measure from photo m2\n",
    "\n",
    "#Basile\n",
    "Cd = 0.8 \n",
    "S = 0.48 \n",
    "\n",
    "#track['filt_airspeed'] = apply_filter(track['airspeed'], 2, 0.08)\n",
    "#13% for CD; 10% for area\n",
    "spd_avrg = track['airspeed'].mean()\n",
    "spd_std = track['airspeed'].std()\n",
    "crouch_start = spd_avrg\n",
    "crouch_full = spd_avrg + 1 * spd_std #experienced bikers will crouch sooner, at 1 std\n",
    "crouch_full_Cd_mult = 0.87\n",
    "crouch_full_S_mult = 0.9\n",
    "crouch_cd_interp = interpolate.interp1d([crouch_start, crouch_full], [Cd, Cd * crouch_full_Cd_mult])\n",
    "crouch_S_interp = interpolate.interp1d([crouch_start, crouch_full], [Cd, Cd * crouch_full_S_mult])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crouch_S(1, crouch_S_interp, crouch_start, crouch_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crouch_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track['Cd'] = track.apply(lambda x: Cd * crouch_cd(x['airspeed'], crouch_cd_interp, crouch_start, crouch_full), axis=1)\n",
    "track['S'] = track.apply(lambda x: S * crouch_S(x['airspeed'], crouch_S_interp, crouch_start, crouch_full), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track['S'].iloc[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track['drag'] = 0.5 * rho * track['airspeed']**2 * track['Cd'] * track['S']\n",
    "track['drag_work'] = track['drag'] * track['dist'] #J\n",
    "track['drag_no_wind'] = 0.5 * rho * track['gs']**2 * Cd * S\n",
    "track['drag_work_no_wind'] = track['drag_no_wind'] * track['dist'] #J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculates the potential energy work\n",
    "\n",
    "Todo: interface for cyclist and bike mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyclist_mass = 85 #kg\n",
    "bike_mass = 12 #kg\n",
    "\n",
    "Crr_tire = 0.003\n",
    "#http://www.biketechreview.com/tires_old/images/AFM_tire_testing_rev9.pdf\n",
    "#https://www.bicyclerollingresistance.com/\n",
    "\n",
    "Crr_tire_std = 1 * Crr_tire #average increase due to road not being 100% smooth\n",
    "\n",
    "mass = cyclist_mass + bike_mass #kg\n",
    "\n",
    "#rolling resistance model\n",
    "g = 9.8 #m/s2\n",
    "Fn = mass * g #normal force\n",
    "Frr_1_tire = Crr_tire_std * Fn\n",
    "Frr = Frr_1_tire * 2\n",
    "track['rr_work'] = Frr * track['dist']\n",
    "track['rr_power'] = Frr * track['gs']\n",
    "\n",
    "\n",
    "#track['filt_delta_elev'] = apply_filter(track['delta_elev'], 2, 0.04)\n",
    "track['pot_work'] = mass * g * track['delta_elev'] #J\n",
    "track['total_work'] = track['drag_work'] + track['pot_work'] + track['rr_work']#J\n",
    "track['total_work_no_wind'] = track['drag_work_no_wind'] + track['pot_work'] + track['rr_work']#J\n",
    "track.drop(track['delta_t'].loc[track['delta_t'] == 0].index, inplace=True) #remove delta_t == 0\n",
    "track['power'] = track['total_work'] / (track['delta_t'].values)\n",
    "track['power_no_wind'] = track['total_work_no_wind'] / (track['delta_t'].values)\n",
    "track.fillna(value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(track.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate averages and basic comparisons..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_work = track['total_work'].sum()\n",
    "pos_power_filter = track['total_work'] >= 0\n",
    "print(f'Total work was {total_work:1.0f} Joules = {total_work * 0.000239006:1.0f} kcal')\n",
    "workout_time = track['ts'].iloc[-1]-track['ts'].iloc[0]\n",
    "print('Workout time was {}, or {:1.0f} seconds'.format(workout_time, workout_time.total_seconds()))\n",
    "print('Average overall power was {:1.0f} Watts'.format(total_work / workout_time.total_seconds()))\n",
    "print('Average positive power was {:1.0f} Watts'.format(track[track['total_work'] >= 0]['total_work'].sum() / workout_time.total_seconds()))\n",
    "print('Measured average power was {:1.0f} Watts'.format(track['m_power'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.cut(track['airspeed'], 10)\n",
    "a.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "print('execution time was {:0.1f} seconds'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need tp filter power...\n",
    "\n",
    "track['filt_power'] = apply_filter(track['power'], 4, 0.04)\n",
    "track['filt_power_no_wind'] = apply_filter(track['power_no_wind'], 4, 0.04)\n",
    "'''\n",
    "#quick check\n",
    "plot_list = ['delta_elev', 'drag_work', 'pot_work', 'total_work', 'delta_t', 'power', 'filt_power']\n",
    "plot_len = len(plot_list)\n",
    "plt.rcParams['figure.figsize'] = [16, 12]\n",
    "f, axarr = plt.subplots(plot_len, sharex=True)\n",
    "for plt_idx, plot_label in enumerate(plot_list):\n",
    "    axarr[plt_idx].plot(track['ts'], track[plot_label])\n",
    "    axarr[plt_idx].set(ylabel=plot_label,\n",
    "                      title=plot_label)\n",
    "    axarr[plt_idx].grid()\n",
    "#fig.savefig(\"test.png\")\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [16, 12]\n",
    "f, axarr = plt.subplots(4, sharex=True)\n",
    "axarr[0].plot((track['ts']),track['filt_power'])\n",
    "\n",
    "axarr[0].set(ylabel='Filtered Power [W]',\n",
    "       title='Power Curve')\n",
    "axarr[0].grid()\n",
    "\n",
    "axarr[1].plot((track['ts']),track['drag_work'])\n",
    "\n",
    "axarr[1].set(ylabel='drag_work',\n",
    "       title='drag work')\n",
    "axarr[1].grid()\n",
    "\n",
    "axarr[2].plot((track['ts']),track['delta_t'].values)\n",
    "\n",
    "axarr[2].set(ylabel='delta_t',\n",
    "       title='delta_t')\n",
    "axarr[2].grid()\n",
    "\n",
    "axarr[3].plot((track['ts']),(track['gs'])*3.6)\n",
    "\n",
    "axarr[3].set(xlabel='Time', ylabel='gs [km/h]',\n",
    "       title='gs')\n",
    "axarr[3].grid()\n",
    "             \n",
    "\n",
    "#fig.savefig(\"test.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [16, 8]\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot((track['ts']),track['pot_work'])\n",
    "\n",
    "ax.set(xlabel='Time', ylabel='energy [J]',\n",
    "       title='Geopotential Work Curve')\n",
    "ax.grid()\n",
    "\n",
    "#fig.savefig(\"test.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "f, axarr = plt.subplots(4, sharex=True)\n",
    "axarr[0].plot((track['ts']),track['power'], alpha=0.8, linewidth=0.3)\n",
    "\n",
    "axarr[0].set(xlabel='Time', ylabel='Power [W]',\n",
    "       title='Power Curve')\n",
    "axarr[0].set_ylim([0, track['power'].max()])\n",
    "axarr[0].grid()\n",
    "\n",
    "axarr[1].plot((track['ts']),track['elev'])\n",
    "\n",
    "axarr[1].set(xlabel='Time', ylabel='Elevation [m]',\n",
    "       title='Elevation Curve')\n",
    "axarr[1].grid()\n",
    "\n",
    "axarr[2].plot((track['ts']),track['delta_ang'])\n",
    "\n",
    "axarr[2].set(xlabel='Time', ylabel='delta [deg]',\n",
    "       title='Wind Relative Angle Curve')\n",
    "axarr[2].grid()\n",
    "\n",
    "axarr[3].plot((track['ts']),track['delta_elev'])\n",
    "\n",
    "axarr[3].set(xlabel='Time', ylabel='[m]',\n",
    "       title='Delta Elev')\n",
    "axarr[3].grid()\n",
    "             \n",
    "\n",
    "#fig.savefig(\"test.png\")\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [16, 12]\n",
    "f, axarr = plt.subplots(4, sharex=True)\n",
    "\n",
    "color = 'tab:green'\n",
    "axarr[0].fill_between((track['ts']),track['elev'].min(),track['elev'], color=color, alpha=0.3)\n",
    "axarr[0].set_ylabel('Elevation [m]', color=color)\n",
    "#fill_between(x, 0, y1)\n",
    "\n",
    "color = 'tab:red'\n",
    "ax2 = axarr[0].twinx()  # instantiate a second axes that shares the same x-axis\n",
    "ax2.plot((track['ts']),track['filt_power'], alpha=0.8, linewidth=0.3, color=color)\n",
    "\n",
    "ax2.set(xlabel='Time', title='Power and Elevation Curves')\n",
    "ax2.set_ylabel('Filtered Power [W]', color=color)\n",
    "ax2.set_ylim([0, track['filt_power'].max()])\n",
    "ax2.grid()\n",
    "\n",
    "color = 'tab:red'\n",
    "axarr[1].plot((track['ts']),track['drag'], alpha=0.8, linewidth=0.3, color=color)\n",
    "\n",
    "\n",
    "axarr[1].set(title='Drag Curve')\n",
    "axarr[1].set_ylabel(ylabel='Drag [N]', color=color)\n",
    "axarr[1].grid()\n",
    "\n",
    "color = 'tab:blue'\n",
    "gs_kmh = ms2kmh(track['gs'])\n",
    "ax3 = axarr[1].twinx()  # instantiate a second axes that shares the same x-axis\n",
    "ax3.plot((track['ts']), gs_kmh, color=color, linewidth=0.3)\n",
    "ax3.set_ylabel('Ground Speed [km/h]', color=color)\n",
    "\n",
    "axarr[2].plot((track['ts']),ms2kmh(track['headwind_comp']))\n",
    "\n",
    "axarr[2].set(ylabel='head wind [km/h]',\n",
    "       title='Head Wind Curve')\n",
    "axarr[2].grid()\n",
    "\n",
    "axarr[3].plot((track['ts']),track['delta_elev'])\n",
    "\n",
    "axarr[3].set(xlabel='Time', ylabel='[m]',\n",
    "       title='Delta Elev')\n",
    "axarr[3].grid()\n",
    "             \n",
    "\n",
    "#fig.savefig(\"test.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Power Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track[track['total_work'] >= 0]\n",
    "ax = track[track['power']>=0]['power'].plot.hist(bins=30, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Power Bins/Time\n",
    "\n",
    "Let's plot now how many minutes was spent in each power bin.\n",
    "\n",
    "Change the power range and bin size below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_power = 0\n",
    "max_power = 1000\n",
    "power_bin_width = 50\n",
    "\n",
    "\n",
    "power_bins =  range(min_power, max_power, power_bin_width)\n",
    "track['power_bin'] = pd.cut(track['power'], power_bins)\n",
    "((track.groupby(['power_bin']).sum().loc[:,'delta_t'])/60).plot(kind='bar', title='Time [m] spent in power [W]');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do a comparison of power with wind and without\n",
    "track['delta_wind_power'] = track['filt_power'] - track['filt_power_no_wind']\n",
    "track['delta_power'] = track['filt_power'] - track['m_power']\n",
    "#plot_list = ['drag_work', 'drag_work_no_wind', 'pot_work', 'total_work', 'total_work_no_wind', 'filt_power', 'filt_power_no_wind', 'delta_wind_power']\n",
    "plot_list = ['filt_power', 'm_power', 'delta_power']\n",
    "plot_len = len(plot_list)\n",
    "plt.rcParams['figure.figsize'] = [16, 12]\n",
    "f, axarr = plt.subplots(plot_len, sharex=True)\n",
    "for plt_idx, plot_label in enumerate(plot_list):\n",
    "    axarr[plt_idx].plot(track['ts'], track[plot_label])\n",
    "    axarr[plt_idx].set(ylabel=plot_label,\n",
    "                      title=plot_label)\n",
    "    axarr[plt_idx].grid()\n",
    "#fig.savefig(\"test.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
